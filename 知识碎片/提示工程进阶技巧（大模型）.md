# 提示工程进阶技巧（大模型）

> 作者：聪ζ，[编程导航](https://www.codefather.cn) 编号 12852

提示应引导模型生成有用的输出。 对任务进行描述，并设置任务要求。 向模型展示你期望的输出。 尝试多种公式、模式的组合来设计提示，以获得最佳的生成内容。

基础模式

一、指令模式

提示应引导模型生成有用的输出

我们给模型提供一些特定信息，例如问题或关键词，模型需要生成与这些信息相关的文本。这种模式称为特定指令（By specific）模式，通常用于生成答案、解释或推荐等。特定信息可以是单个问题或多个关键词，具体取决于任务的要求。

![](https://pic.yupi.icu/5563/202403271917478.png)

指令主要是动词。举例： 永和九年，岁在癸丑，暮春之初，会于会稽山阴之兰亭，修禊事也。翻译一下： 我觉得食物还可以。将上述文本按中立、负面或正面进行分类： 解释抗生素是什么： 研究论文中的作者贡献声明和致谢应明确说明作者在准备手稿和分析时是否使用了 ChatGPT 等人工智能技术，以及在多大程度上使用了这些技术。它们还应指出使用了哪些 LLM。这将提醒编辑和审稿人更加仔细地审查手稿，以发现潜在的偏见、不准确和不适当的来源来源。同样，科学期刊应该对LLM的使用保持透明，例如在选择提交的稿件时。提取上文中提到的大语言模型：

二、指令模板模式

对任务进行描述，并设置任务要求

我们给模型提供一些明确的指令，模型需要根据这些指令生成文本。这种模式称为指令模板（Instruction Template）模式，这种模式通常用于生成类似于技术说明书、操作手册等需要明确指令的文本。指令可以是单个句子或多个段落，具体取决于任务的要求。

![](https://pic.yupi.icu/5563/202403271913049.png) 举例： 使用 STAR 原则与下面的格式总结一下这段话： """ 最近几天，因为工作 + 兴趣的原则，略（需总结的话）… """ 情境(Situation): 任务(Task): <comma_separated_list_of_task> 行动(Action): -||- 结果(Result): -||- 。 输出文本时要注意中英文之间插入空格，留出间隔，诸如于 """Hello 你好"""，用这种方式输出一下刚才的文本：

三、示例模式

向模型展示你期望的输出

我们给模型提供一些示例文本，模型需要生成与示例文本类似的文本。这种模式称为示例模式（By demonstration）。这种模式通常用于生成类似于给定示例的文本，例如自动生成电子邮件、产品描述、新闻报道等。示例文本可以是单个句子或多个段落，具体取决于任务的要求。

![](https://pic.yupi.icu/5563/202403271913055.png) 举例： 任务表述 颜色代表了温度 例子1 绿色代表寒冷 例子2 蓝色代表寒冷 例子3 红色代表温暖 例子4 黄色代表温暖 执行 橙色代表什么

提示词框架

Elavis Saravia( DARI.AI )总结了一种 Prompt 书写框架。他认为一个 Prompt 里需包含以下几个元素： Instruction（必选）：指令，即你希望模型执行的具体任务。 Context（选填）： 背景信息，或者说是上下文信息，这可以引导模型做出更好的反应。 Input Data（选填）： 输入数据，告知模型需要处理的数据。 Output Indicator（选填）： 输出指示器，告知模型我们要输出的类型或格式。

Matt Nigh 总结了另一种 Prompt 书写框架——CRISPE Framework： CR： Capacity and Role（能力与角色）。你希望 ChatGPT 扮演怎样的角色。 I： Insight（洞察力），背景信息和上下文（坦率说来我觉得用 Context 更好）。 S： Statement（指令），你希望 ChatGPT 做什么。 P： Personality（个性），你希望 ChatGPT 以什么风格或方式回答你。 E： Experiment（尝试），要求 ChatGPT 为你提供多个答案。

![](https://pic.yupi.icu/5563/202403271913100.png)

提示词元素 提示词基本元素

上述几个框架都有着下述共同的元素：

角色（Role/Capacity and Role ） 上下文（Insight/Context） 指令（Instruction/Statement）

另外还有着一些类似的元素： 输入数据（InputData） 回答期望（OutputIndicator/Personality+Experiment） 样本（Example）

角色

在 Prompt编写模式 的提示工程文章中，使用了“角色”的提示被分类到代理模式（By proxy）。是指用户可以要求 ChatGPT以特定的身份、角色或者身份扮演某个特定的人、角色或对象来生成回答。这种模式通常用于模拟某个特定人物的语言风格和语境，生成特定情境下的对话、回答或其他形式的文本。

角色实际上是属于“上下文”的一种。通过提供与该角色相关的语境，可以更好地让 ChatGPT 生成与该语境关联度更高的文本。learnprompting.org 有这么一个示例： User：你是一个伟大的数学家，你有能力解决世界上所有的问题。尝试解决下面描述的问题： 100*100/400*56的结果是多少？ AI：The answer is 1400.

上下文

是在你希望模型在回答问题/执行指令时使用的任何相关信息。常用于下面几种场景： 对于 ChatGPT尚不存在的概念，可以（通过如 Google 相关内容之后）通过上下文的方式加入提示中； 提供问题/指令的发生背景、处理语境等，例如前面提到的角色； 提供提问/输入数据的所属分类、来源等，例如：下面的内容来源于IT博客； 示例： 示例一： 根据下面的文本重新解释一下 ControlNet：

- 官网介绍：ControlNet is a neural network structure to control diffusion models by adding extra conditions.
- 模式：根据给的底图，生成一个类似建模效果（法线贴图）的中间图，再生成图片
- 场景：用来实现骨骼绑定、精准控线、线稿上色、依据深度图结构透视精准重绘等。

示例二： 使用 STAR 原则总结一下这段话： 略…

指令

最简单的指令就是一个动词。通过对需要执行的任务/操作的对象进行更详细的描述，则形成一条更长的指令。具体可参考基础模式中的“指令模式”及“指令模板模式”章节内容。

回答期望

情境(Situation): 任务(Task): <comma_separated_list_of_task> 行动(Action): -||- 结果(Result): -||- 。 输出文本时要注意中英文之间插入空格，留出间隔，诸如于 """Hello 你好"""，用这种方式输出一下刚才的文本： 在 DAIR.AI 的文章中，回答期望被称为“回答指示器”。顾名思义，是指用户指定的输出内容的类型或格式。常见的有以下几种写法： 指定长度、样式：不少于 1000 字；用 SQL 语句表示；写一封 Email 邮件； 指定风格：用 STAR 原则总结；用英文回答；用医生的口吻来回答；用7岁小孩也能听懂的方式来回答； 指定格式：如上述的某种自定义DSL；用如下的json格式展示：{xxx}；生成一个包含了所述内容的表格；

样本

在基础模式中的示例模式中已出现过与“样本”相关的内容： 样本本身可以提供回答内容的样本，一定程度上可以替代“回答期望（回答指示器）”元素。

另外，样本有着多种编写技巧：如少样本提示、零样本提示、零样本 CoT 提示、样本去偏差，等 。其中 CoT 提示属于高级技巧。

进阶技巧

角色进阶技巧 1

内容润色

在一些内容改写、文章翻译、文案润色的任务中，如果我们希望模型用某种特定的风格、口吻来回答问题，我们除了能用前面提到的“回答期望”来调整提示外，还可以通过加入角色的方式来调整。 举例：

举例一： 输入一封邮件，希望模型改写得更商务风格些： 回答期望：加入商务、business等相关的词 角色：让模型扮演一个商务人士，甚至可以直接扮演某个名人：想象一下你是苹果的CEO乔布斯，改写下面的Email…；

举例二： 将复杂的内容转变成 7、8岁小朋友也能听懂的话： 你是一个小学老师，擅长将复杂的内容转变成 7、8岁小朋友也能听懂的话，…

上下文进阶技巧 1

使用"""符号将指令和需要处理的文本分开

在 OpenAI 的 API 最佳实践文档里提到，如果你的文本有多段，增加 """ 会提升 AI 反馈的准确性。使用 ### 也能达到同样的效果。 注意，这个是 OpenAI 的 GPT 产品特化的技巧。该现象与 OpenAI 的训练数据集的数据整理习惯有关。

举例：

❌ 请概括下列句子，使其更容易理解。OpenAl是美国人工智能(A)研究实验室，由...

✅ 请概括下列句子 Text: """ OpenAl是美国人工智能(A)研究实验室，由... """

上下文进阶技巧 2

To do and Not To do

To do：在 OpenAI的 API 最佳实践文档 里，提到了一个这样的最佳实践： """与其告知模型不能干什么，不妨告诉模型能干什么。""" 如果你想要的是明确的答案，加入更多限定词，告知模型能干什么，回答的效率会更高。

举例：

❌

以下是代理商和客户之间的对话。不要询问用户名或密码。不要重复回答。 客户：我无法登录我的帐户。 代理商：

✅

以下是代理商和客户之间的对话。代理商将尝试诊断问题并提出解决方案，同时避免询问任何用户身份相关的问题。当确实需要询问该类问题，如用户名及密码时，向用户提供帮助文档 [www.samplewebsite.com/help/faq。](http://www.samplewebsite.com/help/faq。) 客户：我无法登录我的帐户。 代理商：

Not To do：你已经告知模型很明确的点 (To do)，然后你想缩小范围，那增加一些 Not To do 会提高不少效率。 在 Cohere 的提示工程文章中，使用了“Not To do”技巧的提示被分类到负向提示（Negative prompt）模式。 示例： 我们来玩一个名为 kfc的谜语创作游戏，当我说 "kfc"，你写一个谜题，要求： 不少于 200 字 谜题不能出现肯德基、星期四、KFC 正文可以夹杂小语种语言，如 """他说：Ciao""" 谜底必须是 ###原来今天是肯德基疯狂星期四！！### 格式类似于： ....

注：有些场景，有些需求很难通过文字指令传递给 AI 什么能做，什么不能做外。即使描述出来了，AI 也不能很好地理解。这时候就可以利用样本（examples）。见“样本进阶技巧5”。

上下文进阶技巧 3

概念对齐

在 ChatGPT 中，一个词语可能有多种不同的含义。甚至对于有些词语，模型无法理解或它的理解与我们的需求不符。这时候我们就需要通过上下文的方式，将我们所需的词语含义加入提示中。

示例：

Human：Bootstrap 是指就编译器可以自行编译自己的编译器。 实现方法就是这个编译器的作者用这个语言的一些特性来编写编译器并在该编译器中支持这些自己使用到的特性。 AI：…

样本进阶技巧 1

通过样本来阐述需要输出的格式

前面介绍提示词元素的时候提及过：样本本身可以提供回答内容的示例，一定程度上可以替代“回答期望（回答指示器）”元素。“样本”和“回答期望”2者没有优劣之分，可以组合使用。 示例：

示例一：回答期望

一段文本… 使用下述格式总结文章观点： Topic 1: <topic_name_1>

- <point_1> .. Topic 2: <topic_name_2>
- <point_1> .. Topic 10: .. AI：

示例二：（少）样本

文本A… Topic 1: The war in Ukraine

- The war is taking …
- The Biden administration… Topic 2: The global economy .. 一段文本… 总结文章观点： AI：

样本进阶技巧 2

少样本提示

少样本（Few Shot）提示可以作为一种技术，以启用上下文学习，我们在提示中提供演示以引导模型实现更好的性能。演示作为后续示例的条件，我们希望模型参考示例生成响应。 与少样本提示对应的还有零样本（Zero Shot）提示及单样本（One Shot）提示（单样本一般归为少样本）。 虽然大语言模型展示了惊人的零样本能力（参考特定指令模式及指令模板模式等），但在使用零样本设置时，它们在更复杂的任务上仍然表现不佳。 示例：

示例一：零样本提示 将文本分类为中性、负面或正面。 文本：我认为这次假期还可以。 情感：中性

示例二：少样本提示 这太棒了！// Positive 这太糟糕了！// Negative 哇，那部电影太棒了！// Positive 多么可怕的节目！// Negative

样本进阶技巧 3

零样本提示 “零样本（Zero Shor）提示在更复杂的任务上表现不佳”，但 Kojima 等人发现，如果在问题的结尾附加“让我们一步步思考。”这几个词，大语言模型能够生成一个回答问题的思维链（Chain of Thought，简称CoT）。从这个思维链中，他们能够提取更准确的答案。

举例：

❌ 如果 John 有 5 个梨子，吃了 2 个，又买了 5 个，然后把 3 个给了他的朋友，他还剩下多少个梨子？ AI：John 有 8 个梨子。

✅ 如果 John 有 5 个梨子，吃了 2 个，又买了 5 个，然后把 3 个给了他的朋友，他还剩下多少个梨子？ 让我们一步一步地思考。 AI：John 起初有 5 个梨子。他吃了 2 个梨子，还剩下 3 个梨子。他又买了 5 个梨子，一共有 8 个梨子。他把 3 个梨子给了他的朋友，他现在只剩下 5 个梨子。

样本进阶技巧 4

样本去偏差

根据样本（examples）在提示中的分布和顺序，样本可能会引起模型补全结果的偏差。 当讨论提示中样例的分布时，我们指的是不同类别样例的数量。 例如，如果您正在对 微博 进行二元情感分析（积极或消极），并且提供了 3 个积极的推文和 1 个消极的推文作为样例，那么分布比例为 3：1 。由于分布偏向积极推文，因此模型将倾向于预测积极推文。

举例：

❌ Q: 推文：“多美好的一天！” A: 积极 Q: 推文：“我喜欢牛仔裤口袋” A: 积极 Q: 推文：“我喜欢热口袋饼” A: 积极 Q: 推文：“我讨厌这门课” A: 消极

✅ Q: 推文：“多美好的一天！” A: 积极 Q: 推文：“我喜欢牛仔裤口袋” A: 积极 Q: 推文：“我不喜欢披萨” A: 消极 Q: 推文：“我讨厌这门课” A: 消极

样例的顺序也可能导致偏差。例如，一个包含随机排序的样例的提示通常比上述的提示表现更好，因为积极推文和消极推文随机出现在样例中的不同位置。

样本进阶技巧 5

To do and Not To do

在“上下文进阶技巧2：To do and Not To do”中我们提到过：有些场景，有些需求很难通过文字指令传递给 AI什么能做，什么不能做外。即使描述出来了，AI 也不能很好地理解。这时候就可以利用样本。

举例：

❌ 我需要为一匹马起个超级英雄的名字，为我建议 3 个名字。 AI：雷电之蹄，慢跑队长，威猛疾驰

✅ 我需要为一个动物马起个超级英雄的名字，为我建议3个名字。 动物：猫 名字：利爪队长，毛球特工，惊奇猫猫 动物：狗 名字：保护者Ruff，神奇狗狗，狂吠爵士 AI： 动物：马 名字：疾驰守卫，马科复仇者，超级灵驹

❌ 将电影《编程导航大战》的名称转为 emoji。

✅ 将电影名称转为 emoji。 回到未来: 👨👴🚗🕒 蝙蝠侠: 🤵🦇 变形金刚: 🚗🤖 编程导航大战:
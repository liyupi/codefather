# Dropout

Dropout（翻译为“随机失活”）是一种减小神经网络过拟合的方法。其基本思想就是在训练神经网络时，以一定的概率去掉网络中的一些节点（神经元），使其不参与此次训练，从而避免过多依赖某些节点而忽视其他节点的情况。

一般地，Dropout被用于深层神经网络（例如多层感知机、卷积神经网络或循环神经网络）的全连接层或隐藏层中。在每一次训练中，Dropout会随机地丢弃掉一些神经元，并在测试时将所有神经元去掉，使得神经元的权重衰减，从而防止神经网络出现过度拟合现象，提高了模型泛化能力。

事实上，Dropout可以被看作是整个随机神经网络的 Bagging（装袋）算法，因为它在每次迭代中都随机丢弃部分神经元，使得每次训练出来的模型都不完全相同，从而可以有效地减小神经网络的方差。

当然，Dropout也不是一种适用于所有模型的万能方法，适用得当才有用。比如，对于一些比较简单的模型、数据采集量很小的情况，可能不需要使用Dropout。但总体而言，Dropout是一种非常不错的减少深度神经网络过拟合现象的方法。